{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection\n",
    "\n",
    "__Overview:__ __Vesta Corporation__, seeking the best solutions for __fraud prevention__ industry, and now you are invited to join the challenge.\n",
    "\n",
    "In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vestas real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. \n",
    "\n",
    "__Data Description:__\n",
    "In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n",
    "The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-602696\n",
    "\n",
    "__Transaction Table *__\n",
    "\n",
    "- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
    "- TransactionAMT: transaction payment amount in USD\n",
    "- ProductCD: product code, the product for each transaction \n",
    "- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc. \n",
    "- addr: address\n",
    "- dist: distance\n",
    "- P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked. \n",
    "- D1-D15: timedelta, such as days between previous transaction, etc. \n",
    "- M1-M9: match, such as names on card and address, etc.\n",
    "- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "\n",
    "Categorical Features:\n",
    "- ProductCD\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- Pemaildomain Remaildomain\n",
    "- M1 - M9\n",
    "\n",
    "__Identity Table *__\n",
    "\n",
    "Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \n",
    "They're collected by Vesta’s fraud protection system and digital security partners.\n",
    "(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n",
    "\n",
    "Categorical Features:\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id12 - id38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "* [1.2 Data Analysis](#DataAnalysis)\n",
    "    * [1.2.1 Train Data Exploration](#TrainDataExploration)\n",
    "* [1.3 Preprocessing](#Preprocessing)\n",
    "    * [1.3.1 New columns (Feature ingeniring)](#FeatureIngeniring)\n",
    "* [1.4 Baseline Models](#BaselineModels)\n",
    "* [1.5 Model Tuning](#ModelTuning)\n",
    "    * [1.5.1 Regression analisys](#RegressionAnalisys)\n",
    "    * [1.5.2 XGBoost analisys](#XGBoostAnalisys)\n",
    "    * [1.5.3 Clasterization](#Clasterization)\n",
    "    * [1.5.4 Model Parameters](#ModelParameters)\n",
    "* [1.6 Feature selection](#FeatureSelection)\n",
    "    * [1.6.1 Most Correlated](#MostCorrelated)\n",
    "    * [1.6.2 Most Important](#MostImportant)\n",
    "    * [1.6.2 ks_2samp](#ks_2samp)\n",
    "* [1.7 Kagle Results](#Kaggle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:22:55.330337Z",
     "start_time": "2019-09-18T10:22:54.272539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import timeit\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Visualiazation\n",
    "from seaborn import countplot\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Models and multiclasses support\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "# Model tuning\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:22:55.347313Z",
     "start_time": "2019-09-18T10:22:55.332792Z"
    }
   },
   "outputs": [],
   "source": [
    "def CReduction(df, col_old):\n",
    "    col_new = col_old + '_new'\n",
    "    min_ = df[col_old].min().astype(int)\n",
    "    max_ = df[col_old].max().astype(int)\n",
    "    for i in range(min_, max_):\n",
    "        if df[col_old].value_counts(normalize=True)[:i].sum() > 0.99:\n",
    "            n_cat = i + 10\n",
    "            break\n",
    "\n",
    "    df[col_new] = df[col_old]\n",
    "    df.loc[df[col_new] > n_cat - 1, [col_new]] = n_cat\n",
    "    # try:\n",
    "    #    plot_fts(df, [col_old, col_new], n_cols=2)\n",
    "    # except:\n",
    "    #    n_cat = n_cat\n",
    "\n",
    "    df[col_new].fillna(n_cat).astype(np.int8)\n",
    "\n",
    "    # print(n_cat, df[col_old].value_counts(normalize=True)[:n_cat].sum())\n",
    "    # df[col_old].value_counts(normalize=True)[:3]\n",
    "    return df  # df.drop(col_old, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def TrainAndPredict(X_train, y_train, imputer, scaler, scaler_name, outlier,\n",
    "                    model, model_name, skf):\n",
    "    ''' Function train the model and calculate roc_auc on cross validation '''\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([('imputer', imputer), (scaler_name, scaler),\n",
    "                         ('outlier', outlier), ('smote', smote),\n",
    "                         (model_name, model)])\n",
    "\n",
    "    # Calculate roc_auc on Cross Validation\n",
    "    roc_auc = cross_val_score(\n",
    "        pipeline, X_train, y_train, scoring='roc_auc', cv=skf).mean()\n",
    "\n",
    "    return round(roc_auc, 4)\n",
    "\n",
    "\n",
    "def SecondsToStr(time_taken):\n",
    "    ''' Function return hours, minutes, seconds '''\n",
    "    ''' from the time in string format. '''\n",
    "\n",
    "    hours, rest = divmod(time_taken, 3600)\n",
    "    minutes, seconds = divmod(rest, 60)\n",
    "    h_ = str(math.trunc(hours))\n",
    "    m_ = str(math.trunc(minutes))\n",
    "    s_ = str(round(seconds, 2))\n",
    "    time_taken_str = ':'.join([h_, m_, s_])\n",
    "\n",
    "    # return hours, minutes, seconds from the time taken\n",
    "    return time_taken_str\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    ''' iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    '''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
    "                        np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
    "                        np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
    "                        np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(\n",
    "                        np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
    "                        np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:22:55.366041Z",
     "start_time": "2019-09-18T10:22:55.349589Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fts(df, fts, n_cols=3, m_cat=10):\n",
    "    n_fts = len(fts)\n",
    "    n_rows = -(-n_fts // n_cols)  # ceiling division\n",
    "    to_del = (n_cols - (n_fts % n_cols)) % n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 3 * n_rows))\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.45)\n",
    "    axes = (axes if type(axes) is np.ndarray else np.array(axes)).ravel()\n",
    "    strp = lambda l: f\"{float(l):g}\" if l.replace(\".\", \"\").isdigit() else l[:7]\n",
    "    np_col = df.select_dtypes(np.number).columns\n",
    "    for i, ft in enumerate(fts):\n",
    "        ax1 = axes[i]\n",
    "        ax2 = ax1.twinx()\n",
    "        if ft in np_col:\n",
    "            cut, bins = pd.cut(df[ft], 10, retbins=True, right=False)\n",
    "            ax1.hist(\n",
    "                df[ft],\n",
    "                bins,\n",
    "                color=\"#A8DBA8\",\n",
    "                edgecolor=\"k\",\n",
    "                log=True,\n",
    "                zorder=2)\n",
    "            ax1.tick_params(\"x\", pad=2)\n",
    "            ax1.grid(zorder=0)\n",
    "            ctrs = (bins[:-1] + bins[1:]) / 2\n",
    "            vals = df.groupby(cut)[\"isFraud\"].mean()\n",
    "            ax2.plot(ctrs, vals, marker=\"o\", c=\"#0B486B\", lw=2)\n",
    "        else:\n",
    "            cts = df[ft].value_counts().nlargest(m_cat)\n",
    "            ax1.bar(\n",
    "                cts.index,\n",
    "                cts.values,\n",
    "                width=1,\n",
    "                color=\"#a6cee3\",\n",
    "                edgecolor=\"k\",\n",
    "                log=True,\n",
    "                zorder=2)\n",
    "            ax1.tick_params(\"x\", labelrotation=45, pad=0)\n",
    "            ax1.grid(zorder=0)\n",
    "            vals = df.groupby(ft)[\"isFraud\"].mean().loc[cts.index]\n",
    "            ax2.plot(vals, marker=\"o\", c=\"#0B486B\", lw=2)\n",
    "            ax2.set_xticklabels(list(map(strp, cts.index)))\n",
    "        ax1.minorticks_off()\n",
    "        ax2.minorticks_off()\n",
    "        ax1.set_ylim(1, 10**6)\n",
    "        ax1.set_title(ft, loc=\"right\")\n",
    "    # ax2.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n",
    "    for i in range(to_del, 0, -1):\n",
    "        fig.delaxes(axes[-i])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator,\n",
    "                        title,\n",
    "                        X,\n",
    "                        y,\n",
    "                        ylim=None,\n",
    "                        cv=None,\n",
    "                        n_jobs=1,\n",
    "                        train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    ''' Generate a simple plot of the test and training learning curve. '''\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\")\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color='g')\n",
    "    plt.plot(\n",
    "        train_sizes,\n",
    "        train_scores_mean,\n",
    "        'o-',\n",
    "        color='r',\n",
    "        label='Training score')\n",
    "    plt.plot(\n",
    "        train_sizes,\n",
    "        test_scores_mean,\n",
    "        'o-',\n",
    "        color='g',\n",
    "        label='Cross-validation score')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:22:55.385524Z",
     "start_time": "2019-09-18T10:22:55.368086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=66)\n",
    "\n",
    "    X, y = tr_df[features_columns], tr_df[target]\n",
    "    # P,P_y = tt_df[features_columns], tt_df[target]\n",
    "    P = tt_df[features_columns]\n",
    "\n",
    "    tt_df = tt_df[['TransactionID']]\n",
    "    predictions = np.zeros(len(tt_df))\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:', fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx, :], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx, :], y[val_idx]\n",
    "\n",
    "        print(len(tr_x), len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets=[tr_data, vl_data],\n",
    "            verbose_eval=200)\n",
    "\n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p / NFOLDS\n",
    "\n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "\n",
    "    tt_df['prediction'] = predictions\n",
    "\n",
    "    return tt_df, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:22:55.390178Z",
     "start_time": "2019-09-18T10:22:55.387652Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed everything for deterministic results\n",
    "random.seed(66)\n",
    "os.environ['PYTHONHASHSEED'] = str(66)\n",
    "np.random.seed(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:23:36.100798Z",
     "start_time": "2019-09-18T10:22:55.392173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0:0:40.7\n"
     ]
    }
   ],
   "source": [
    "# import Datasets\n",
    "start_time = time.time()\n",
    "\n",
    "train_df = pd.read_csv('input_files/train_transaction.csv')\n",
    "test_df = pd.read_csv('input_files/test_transaction.csv')\n",
    "train_identity = pd.read_csv('input_files/train_identity.csv')\n",
    "test_identity = pd.read_csv('input_files/test_identity.csv')\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis <a class=\"anchor\" id=\"DataAnalysis\"></a>\n",
    "\n",
    "### Train Data Exploration <a class=\"anchor\" id=\"TrainDataExploration\"></a>\n",
    "\n",
    "very good data exploration and vizualization is here: \n",
    "\n",
    "https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n",
    "      \n",
    "https://www.kaggle.com/jackdry/ieee-fraud-detection-exploratory-data-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:23:36.125715Z",
     "start_time": "2019-09-18T10:23:36.103131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (590540, 394)\n",
      "\n",
      "75.58% train transactions have no corresponding identity information\n",
      "\n",
      "Types of the columns: float64    376\n",
      "object      14\n",
      "int64        4\n",
      "dtype: int64\n",
      "Data shape: (506691, 393)\n",
      "\n",
      "71.99% test transactions have no corresponding identity information\n",
      "\n",
      "Types of the columns: float64    376\n",
      "object      14\n",
      "int64        3\n",
      "dtype: int64\n",
      "Numerical columns 378, categorical columns 14\n"
     ]
    }
   ],
   "source": [
    "# some information about train dataset..\n",
    "print('Data shape:', train_df.shape)\n",
    "print('\\n{}% train transactions have no corresponding identity information'.format(round(100 * (train_df.shape[0] - train_identity.shape[0])/train_df.shape[0], 2)))\n",
    "print('\\nTypes of the columns:', train_df.dtypes.value_counts())\n",
    "\n",
    "# some information about test dataset..\n",
    "print('Data shape:', test_df.shape)\n",
    "print('\\n{}% test transactions have no corresponding identity information'.format(round(100 * (test_df.shape[0] - test_identity.shape[0])/test_df.shape[0], 2)))\n",
    "print('\\nTypes of the columns:', test_df.dtypes.value_counts())\n",
    "\n",
    "cat_cols = []\n",
    "num_cols = []\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == object:\n",
    "        cat_cols.append(col)\n",
    "    elif ((col != 'isFraud') and (col != 'TransactionID')):\n",
    "        num_cols.append(col)\n",
    "print('Numerical columns {}, categorical columns {}'.format(len(num_cols), len(cat_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:23:36.436649Z",
     "start_time": "2019-09-18T10:23:36.128608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 394)\n",
      "Share of the classes:\n",
      "0    96.5\n",
      "1     3.5\n",
      "Name: isFraud, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f82aa92b908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "\n",
    "# Check the share of the classes\n",
    "print('Share of the classes:')\n",
    "print(round(train_df['isFraud'].value_counts(normalize=True)*100, 2))\n",
    "countplot(x='isFraud', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:23:36.442376Z",
     "start_time": "2019-09-18T10:23:36.438919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Crossvalidation StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Outlier\n",
    "outlier = RobustScaler()\n",
    "# Average instead of the missing values\n",
    "imputer = Imputer(missing_values=np.NAN, axis=1)\n",
    "# Oversampling for the class balansing\n",
    "smote = SMOTE(random_state=1)\n",
    "# Scaler\n",
    "scaler = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:23:36.449115Z",
     "start_time": "2019-09-18T10:23:36.444520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''# оценим как много объектов действительно нужно для построения качественной модели. \n",
    "# Для обучения доступна достаточно большая выборка и может так оказаться, \n",
    "# что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. \n",
    "# Построим кривые обучения, обучая модель на выборках разного размера \n",
    "# начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('scaler', scaler),\n",
    "        ('outlier', outlier),\n",
    "        ('smote', smote),\n",
    "        ('XGBoost', XGBClassifier(random_state=1))\n",
    "    ])\n",
    "plot_learning_curve(pipeline, 'Learning Curves (XGBClassifier)', X_train, y_train, cv=skf)\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))\n",
    "\n",
    "plt.show()'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing <a class=\"anchor\" id=\"Preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T12:28:14.371407Z",
     "start_time": "2019-09-03T12:30:07.087Z"
    }
   },
   "source": [
    "### New columns (Feature' ingeniring) <a class=\"anchor\" id=\"FeatureIngeniring\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:25:12.996232Z",
     "start_time": "2019-09-18T10:23:36.451100Z"
    }
   },
   "outputs": [],
   "source": [
    "# C1-C14 analisys + New Features\n",
    "# C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. \n",
    "# The actual meaning is masked. \n",
    "old_C_list = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n",
    "new_C_list = ['C1_new','C2_new','C3_new','C4_new','C5_new','C6_new','C7_new','C8_new','C9_new','C10_new','C11_new','C12_new','C13_new','C14_new']\n",
    "for col in old_C_list :\n",
    "    # print(col)\n",
    "    train_df = CReduction(train_df, col)\n",
    "    test_df = CReduction(test_df, col)\n",
    "    \n",
    "\n",
    "# add sum of the all columns\n",
    "for df in [train_df , test_df]:\n",
    "    df['C_Sum']  = df[new_C_list].sum(axis=1).astype(np.int8)\n",
    "    df['C_NA'] = df[new_C_list].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:25:15.019523Z",
     "start_time": "2019-09-18T10:25:12.998935Z"
    }
   },
   "outputs": [],
   "source": [
    "# M1-M9 New Features\n",
    "# M1-M9: match, such as names on card and address, etc.\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n",
    "    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:25:21.953558Z",
     "start_time": "2019-09-18T10:25:15.021775Z"
    }
   },
   "outputs": [],
   "source": [
    "# TransactionDT New Features\n",
    "# TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
    "# Let's add temporary \"time variables\" for aggregations\n",
    "# and add normal \"time variables\"\n",
    "\n",
    "reference_datetime = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "reference_year = 2017\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    # Temporary\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (reference_datetime + datetime.timedelta(seconds=x)))\n",
    "    \n",
    "    df['DT_M'] = (df['DT'].dt.year - reference_year) * 12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year - reference_year) * 52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year - reference_year) * 365 + df['DT'].dt.dayofyear\n",
    "\n",
    "    df['DT_hour'] = df['DT'].dt.hour\n",
    "    df['DT_day_week'] = df['DT'].dt.dayofweek\n",
    "    df['DT_day'] = df['DT'].dt.day\n",
    "\n",
    "    # Drop technical column\n",
    "    df.drop('DT', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:25:23.536411Z",
     "start_time": "2019-09-18T10:25:21.955589Z"
    }
   },
   "outputs": [],
   "source": [
    "# D9 column\n",
    "# D1-D15: timedelta, such as days between previous transaction, etc. \n",
    "for df in [train_df, test_df]:\n",
    "    df['D9'] = np.where(df['D9'].isna(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:25:23.828250Z",
     "start_time": "2019-09-18T10:25:23.538436Z"
    }
   },
   "outputs": [],
   "source": [
    "# card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "# Reset values for \"noisy\" card1\n",
    "for col in ['card1']:\n",
    "    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    valid_card = valid_card[col].value_counts()\n",
    "    valid_card = valid_card[valid_card > 2]\n",
    "    valid_card = list(valid_card.index)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col] = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "    test_df[col] = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:25:27.622136Z",
     "start_time": "2019-09-18T10:25:23.831183Z"
    }
   },
   "outputs": [],
   "source": [
    "# ProductCD and M4 Target mean\n",
    "for col in ['ProductCD', 'M4']:\n",
    "    temp_dict = train_df.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col + '_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col + '_target_mean'].to_dict()\n",
    "\n",
    "    train_df[col + '_target_mean'] = train_df[col].map(temp_dict)\n",
    "    test_df[col + '_target_mean'] = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:26:39.673118Z",
     "start_time": "2019-09-18T10:25:27.625684Z"
    }
   },
   "outputs": [],
   "source": [
    "# TransactionAmt\n",
    "\n",
    "# Let's add some kind of client uID based on cardID ad addr columns\n",
    "# The value will be very specific for each client so we need to remove it\n",
    "# from final feature. But we can use it for aggregations.\n",
    "train_df['uid'] = train_df['card1'].astype(str) + '_' + train_df['card2'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str) + '_' + test_df['card2'].astype(\n",
    "    str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str) + '_' + train_df[\n",
    "    'card3'].astype(str) + '_' + train_df['card5'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str) + '_' + test_df['card3'].astype(\n",
    "    str) + '_' + test_df['card5'].astype(str)\n",
    "\n",
    "train_df['uid3'] = train_df['uid2'].astype(str) + '_' + train_df[\n",
    "    'addr1'].astype(str) + '_' + train_df['addr2'].astype(str)\n",
    "test_df['uid3'] = test_df['uid2'].astype(str) + '_' + test_df['addr1'].astype(\n",
    "    str) + '_' + test_df['addr2'].astype(str)\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values\n",
    "train_df['TransactionAmt_check'] = np.where(\n",
    "    train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
    "test_df['TransactionAmt_check'] = np.where(\n",
    "    test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise\n",
    "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "# (even if features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['card1', 'card2', 'card3', 'card5', 'uid', 'uid2', 'uid3']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        new_col_name = col + '_TransactionAmt_' + agg_type\n",
    "        temp_df = pd.concat([\n",
    "            train_df[[col, 'TransactionAmt']], test_df[[col, 'TransactionAmt']]\n",
    "        ])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg(\n",
    "            [agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "\n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()\n",
    "\n",
    "        train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "        test_df[new_col_name] = test_df[col].map(temp_df)\n",
    "\n",
    "# Small \"hack\" to transform distribution\n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score\n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "train_df['TransactionAmt'] = np.log1p(train_df['TransactionAmt'])\n",
    "test_df['TransactionAmt'] = np.log1p(test_df['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:26:41.348631Z",
     "start_time": "2019-09-18T10:26:39.675473Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'P_emaildomain' - 'R_emaildomain'\n",
    "p = 'P_emaildomain'\n",
    "r = 'R_emaildomain'\n",
    "uknown = 'email_not_provided'\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df[p] = df[p].fillna(uknown)\n",
    "    df[r] = df[r].fillna(uknown)\n",
    "\n",
    "    # Check if P_emaildomain matches R_emaildomain\n",
    "    df['email_check'] = np.where((df[p] == df[r]) & (df[p] != uknown), 1, 0)\n",
    "\n",
    "    df[p + '_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n",
    "    df[r + '_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "## Local test doesn't show any boost here,\n",
    "## but I think it's a good option for model stability\n",
    "\n",
    "## Also, we will do frequency encoding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:26:43.836501Z",
     "start_time": "2019-09-18T10:26:41.350754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device info\n",
    "\n",
    "for df in [train_identity, test_identity]:\n",
    "    # Device info\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    # Device info 2\n",
    "    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    # Browser\n",
    "    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:26:57.208009Z",
     "start_time": "2019-09-18T10:26:43.838686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge Identity columns\n",
    "temp_df = train_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "train_df = pd.concat([train_df,temp_df], axis=1)\n",
    "    \n",
    "temp_df = test_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "test_df = pd.concat([test_df,temp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:27:40.230860Z",
     "start_time": "2019-09-18T10:26:57.210082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freq encoding\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "          'id_30','id_30_device','id_30_version',\n",
    "          'id_31_device',\n",
    "          'id_33',\n",
    "          'uid','uid2','uid3',\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n",
    "    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "\n",
    "\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train_df[col+'_total'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_total']  = test_df[col].map(fq_encode)\n",
    "        \n",
    "\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['uid']\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp_df = pd.concat([train_df[[col,period]], test_df[[col,period]]])\n",
    "        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n",
    "        fq_encode = temp_df[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train_df[new_column] = (train_df[col].astype(str) + '_' + train_df[period].astype(str)).map(fq_encode)\n",
    "        test_df[new_column]  = (test_df[col].astype(str) + '_' + test_df[period].astype(str)).map(fq_encode)\n",
    "        \n",
    "        train_df[new_column] /= train_df[period+'_total']\n",
    "        test_df[new_column]  /= test_df[period+'_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:29:35.389936Z",
     "start_time": "2019-09-18T10:27:40.233923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0:1:55.14\n"
     ]
    }
   ],
   "source": [
    "# Categorical Treatment\n",
    "start_time = time.time()\n",
    "# заменить все редкие хэши на слово Rare (редкие = встречаются реже 0.5%),\n",
    "# заменить все пропущенные данные на слово Missing и развернуть как бинарную переменную\n",
    "\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype == 'O':\n",
    "        # print(col)\n",
    "\n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "\n",
    "        train_df[col].fillna('Missing', inplace=True)\n",
    "        test_df[col].fillna('Missing', inplace=True)\n",
    "\n",
    "        thr = 0.005\n",
    "        df = train_df[col].append(test_df[col])\n",
    "        df[col] = train_df[col].append(test_df[col])\n",
    "        d = dict(df[col].value_counts(dropna=False) / len(df[col]))\n",
    "\n",
    "        train_df[col] = train_df[col].apply(\n",
    "            lambda x: 'Rare' if d[x] <= thr else x)\n",
    "        test_df[col] = test_df[col].apply(\n",
    "            lambda x: 'Rare' if d[x] <= thr else x)\n",
    "\n",
    "        # train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        # test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "\n",
    "        # train_df[col] = train_df[col].astype(str)\n",
    "        # test_df[col] = test_df[col].astype(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col]) + list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2203.17 MB\n",
      "Memory usage after optimization is: 645.41 MB\n",
      "Decreased by 70.7%\n",
      "Memory usage of dataframe is 1890.35 MB\n"
     ]
    }
   ],
   "source": [
    "# Reduce mem_usage\n",
    "\n",
    "num_cols = []\n",
    "for col in test_df.columns:\n",
    "    if (test_df[col].dtype == float) or (test_df[col].dtype == int):\n",
    "        num_cols.append(col)\n",
    "\n",
    "start_time = time.time()\n",
    "train_df[num_cols] = reduce_mem_usage(train_df[num_cols])\n",
    "\n",
    "test_df[num_cols] = reduce_mem_usage(test_df[num_cols])\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.486Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train_df['isFraud']\n",
    "X_train = train_df.drop('isFraud', axis=1)\n",
    "\n",
    "X_test = test_df\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models <a class=\"anchor\" id=\"BaselineModels\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.501Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Try different types of Classifiers\n",
    "baseline_models = [\n",
    "     ('LogRegression   ', LogisticRegression(random_state=1)),\n",
    "     ('Ridge           ', RidgeClassifier(random_state=1)),\n",
    "     ('RandomForest    ', RandomForestClassifier(random_state=1)),\n",
    "     ('GradientBoosting', GradientBoostingClassifier(random_state=1)),\n",
    "     ('XGBoost         ', XGBClassifier(random_state=1)),\n",
    "]\n",
    "\n",
    "best_roc_auc = 0\n",
    "for model_name, model in baseline_models:\n",
    "    roc_auc = TrainAndPredict(X_train[:15000], y_train[:15000], imputer, scaler, 'scaler', outlier, model, model_name, skf)\n",
    "    print(model_name, roc_auc)\n",
    "    if roc_auc > best_roc_auc:\n",
    "        best_model = model\n",
    "        best_model_name = model_name\n",
    "        best_roc_auc = roc_auc\n",
    "        \n",
    "print('\\nBest model is {} best score is {}'.format(best_model_name, best_roc_auc))\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison with lightgbm\n",
    "\n",
    "# Model params\n",
    "lgb_params = {\n",
    "                    'objective': 'binary',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'metric': 'auc',\n",
    "                    'n_jobs': -1,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth': -1,\n",
    "                    'tree_learner': 'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq': 1,\n",
    "                    'subsample': 0.7,\n",
    "                    'n_estimators': 800,\n",
    "                    'max_bin': 255,\n",
    "                    'verbose': -1,\n",
    "                    'seed': 66,\n",
    "                }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gbm = lgb.train(lgb_params, lgb.Dataset(X_train[:15000], y_train[:15000]))\n",
    "\n",
    "y_pred = gbm.predict(X_train[15001:], num_iteration=gbm.best_iteration)\n",
    "\n",
    "print('The roc_auc_score of prediction is:', round(roc_auc_score(y_train[15001:], y_pred), 4))\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.523Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importances\n",
    "# print('Feature importances:', list(gbm.feature_importance()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning <a class=\"anchor\" id=\"ModelTuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analisys <a class=\"anchor\" id=\"RegressionAnalisys\"></a>\n",
    "\n",
    "https://habr.com/ru/post/270367/\n",
    "\n",
    "Регрессия в качестве регрессора\n",
    "\n",
    "\n",
    "Следующий шаг уже не такой банальный. Из распределения классов я предположил, что возрастные группы упорядочены, то есть 0<1…<6 или наоборот. А раз так, то можно не классифицировать, а строить регрессию. Она будет работать плохо, но зато её результат можно передать другим алгоритмам для обучения. Поэтому запускаем обычную линейную регрессию с функцией потерь huber и оптимизируем её через стохастический градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.535Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sgd_pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('Normalizer', Normalizer()),\n",
    "        ('outlier', outlier),\n",
    "        ('smote', smote),\n",
    "        ('sgd', SGDRegressor(loss='huber', n_iter_no_change=100))\n",
    "    ])\n",
    "sgd_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# New features from regression' analisys\n",
    "X_train = pd.concat([X_train, pd.DataFrame(sgd_pipeline.predict(X_train)[None].T, columns=['sgd_prediction'])], axis=1)\n",
    "X_test = pd.concat([X_test, pd.DataFrame(sgd_pipeline.predict(X_test)[None].T, columns=['sgd_prediction'])], axis=1)\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost analisys <a class=\"anchor\" id=\"XGBoostAnalisys\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.546Z"
    }
   },
   "outputs": [],
   "source": [
    "''' start_time = time.time()\n",
    "\n",
    "XGB_pipeline = Pipeline([\n",
    "    ('imputer', imputer), \n",
    "    ('Normalizer', Normalizer()),\n",
    "    ('outlier', outlier), \n",
    "    ('smote', smote),\n",
    "    ('XGB', XGBClassifier(silent=False, nthread=4, max_depth=10, n_estimators=800, subsample=0.5, learning_rate=0.03, seed=66, random_state=1))])\n",
    "\n",
    "XGB_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# New features from regression' analisys\n",
    "X_train = pd.concat([X_train, pd.DataFrame(XGB_pipeline.predict(X_train)[None].T, columns=['XGB_prediction'])], axis=1)\n",
    "X_test = pd.concat([X_test, pd.DataFrame(XGB_pipeline.predict(X_test)[None].T, columns=['XGB_prediction'])], axis=1)\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time))) '''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasterization <a class=\"anchor\" id=\"Clasterization\"></a>\n",
    "\n",
    "https://habr.com/ru/post/270367/\n",
    "\n",
    "Вторая интересная мысль, которую я пробовал: кластеризация данных методом k-средних. Если в данных есть реальная структура (а в данных по абонентам она должна быть), то k-средних её почувствует. Сначала я взял k=7, потом добавил 3 и 15 (в два раза больше и в два раза меньше). Предсказания каждого из этих алгоритмов — номера кластеров для каждого образца. Так как эти номера не упорядочены, то оставить их числами нельзя, надо обязательно бинаризовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.564Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clasters\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_scaler = Normalizer()\n",
    "best_scaler_name = 'Normalizer'\n",
    "\n",
    "k_models = [\n",
    "     ('k10', KMeans(n_clusters=10, precompute_distances=True, n_jobs=-1)),\n",
    "     ('k7', KMeans(n_clusters=7, precompute_distances=True, n_jobs=-1)),\n",
    "     ('k6', KMeans(n_clusters=6, precompute_distances=True, n_jobs=-1)),\n",
    "     ('k5', KMeans(n_clusters=5, precompute_distances=True, n_jobs=-1)),\n",
    "     ('k4', KMeans(n_clusters=4, precompute_distances=True, n_jobs=-1)),\n",
    "     ('k3', KMeans(n_clusters=3, precompute_distances=True, n_jobs=-1)),\n",
    "     ('k2', KMeans(n_clusters=2, precompute_distances=True, n_jobs=-1))\n",
    "]\n",
    "\n",
    "for k_name, k_model in k_models:\n",
    "    print(k_name)\n",
    "    k_pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('Normalizer', Normalizer()),\n",
    "        (k_name, k_model)\n",
    "    ])\n",
    "    k_pipeline.fit(X_train)\n",
    "    # New features from clusterization\n",
    "    X_train = pd.concat([X_train, pd.DataFrame(k_pipeline.predict(X_train)[None].T, columns=[k_name + '_prediction'])], axis=1)\n",
    "    X_test = pd.concat([X_test, pd.DataFrame(k_pipeline.predict(X_test)[None].T, columns=[k_name + '_prediction'])], axis=1)\n",
    "\n",
    "# binarization (+25 new features)\n",
    "X_train = pd.get_dummies(X_train, columns=['k10_prediction', 'k7_prediction', 'k6_prediction', 'k5_prediction', 'k4_prediction', 'k3_prediction', 'k2_prediction'])\n",
    "X_test = pd.get_dummies(X_test, columns=['k10_prediction', 'k5_prediction', 'k7_prediction', 'k6_prediction', 'k4_prediction', 'k3_prediction', 'k2_prediction'])\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters <a class=\"anchor\" id=\"ModelParameters\"></a>\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "    \n",
    "__For Better Accuracy__\n",
    "\n",
    "- Use large max_bin (may be slower)\n",
    "- Use small learning_rate with large num_iterations\n",
    "- Use large num_leaves (may cause over-fitting)\n",
    "- Use bigger training data\n",
    "- Try dart\n",
    "\n",
    "__Deal with Over-fitting__\n",
    "\n",
    "- Use small max_bin\n",
    "- Use small num_leaves\n",
    "- Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "- Use bigger training data\n",
    "- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
    "- Try max_depth to avoid growing deep tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.574Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_roc_auc = 0\n",
    "#Try dart\n",
    "for boosting_type in ['gbdt', 'dart']:\n",
    "    #Use small learning_rate with large num_iterations\n",
    "    for learning_rate in [0.005]:#[0.001, 0.005, 0.005, 0.05]:\n",
    "        for n_estimators in [1800]:#[3000, 2000, 1000]:\n",
    "            #Use large num_leaves (may cause over-fitting)\n",
    "            for num_leaves in [2**10]:#, 2**8, 2**10]:\n",
    "   \n",
    "                params = {\n",
    "                    'boosting_type': boosting_type,\n",
    "                    'num_leaves': num_leaves,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'n_estimators':  n_estimators,\n",
    "                }\n",
    "\n",
    "                gbm = lgb.train(params, lgb.Dataset(X_train[:15000], y_train[:15000]))\n",
    "                y_pred = gbm.predict(X_train[15001:], num_iteration=gbm.best_iteration)\n",
    "                roc_auc = roc_auc_score(y_train[15001:], y_pred)\n",
    "                print('boosting_type: {} num_leaves: {} learning_rate: {} n_estimators: {} roc_auc: {}'.format(boosting_type, num_leaves, learning_rate, n_estimators, roc_auc))\n",
    "\n",
    "                if roc_auc > best_roc_auc:\n",
    "                    best_boosting_type = boosting_type\n",
    "                    best_num_leaves = num_leaves\n",
    "                    best_learning_rate = learning_rate\n",
    "                    best_n_estimators = n_estimators\n",
    "                    best_roc_auc = roc_auc\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.584Z"
    }
   },
   "outputs": [],
   "source": [
    "'''start_time = time.time()\n",
    "\n",
    "best_roc_auc = 0\n",
    "#Try dart\n",
    "for boosting_type in ['gbdt', 'dart']:\n",
    "    #Use small learning_rate with large num_iterations\n",
    "    for learning_rate in [0.001, 0.005, 0.005, 0.05]:\n",
    "        for n_estimators in [3000, 2000, 1000]:\n",
    "            #Use large num_leaves (may cause over-fitting)\n",
    "            for num_leaves in [2**6, 2**8, 2**10]:\n",
    "   \n",
    "                params = {\n",
    "                    'boosting_type': boosting_type,\n",
    "                    'num_leaves': num_leaves,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'n_estimators':  n_estimators,\n",
    "                }\n",
    "\n",
    "                gbm = lgb.train(params, lgb.Dataset(X_train[:15000], y_train[:15000]))\n",
    "                y_pred = gbm.predict(X_train[15001:], num_iteration=gbm.best_iteration)\n",
    "                roc_auc = roc_auc_score(y_train[15001:], y_pred)\n",
    "                print('boosting_type: {} num_leaves: {} learning_rate: {} n_estimators: {} roc_auc: {}'.format(boosting_type, num_leaves, learning_rate, n_estimators, roc_auc))\n",
    "\n",
    "                if roc_auc > best_roc_auc:\n",
    "                    best_boosting_type = boosting_type\n",
    "                    best_num_leaves = num_leaves\n",
    "                    best_learning_rate = learning_rate\n",
    "                    best_n_estimators = n_estimators\n",
    "                    best_roc_auc = roc_auc\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection <a class=\"anchor\" id=\"FeatureSelection\"></a>\n",
    "\n",
    "### Most Correlated <a class=\"anchor\" id=\"MostCorrelated\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.593Z"
    }
   },
   "outputs": [],
   "source": [
    "'''# Correlation (for the numerical colimns)\n",
    "pd.concat([y_train, X_train], axis=1)\n",
    "corr = pd.concat([y_train, X_train], axis=1).corr()\n",
    "\n",
    "# Correlation to target variable\n",
    "corr_to_target = abs(corr['isFraud'].iloc[1:].copy())\n",
    "corr_to_target.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "corr_to_target[corr_to_target > 0.30].plot(kind='bar', title='Correlation with target variable (Top correlated features)')\n",
    "\n",
    "# Features with the highest correlation to target\n",
    "correlated_features = corr_to_target[corr_to_target > 0.30].index.tolist()\n",
    "len(correlated_features)\n",
    "\n",
    "# correlated_features\n",
    "# roc_auc = TrainAndPredict(X_train[correlated_features], y_train, imputer, best_scaler, 'scaler', outlier, best_model, best_model_name, skf)\n",
    "# print('Most Correlated: ', roc_auc)'''\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.603Z"
    }
   },
   "outputs": [],
   "source": [
    "# XGB_pipeline.steps[4][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Important <a class=\"anchor\" id=\"MostImportant\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.613Z"
    }
   },
   "outputs": [],
   "source": [
    "'''importances = XGB_pipeline.steps[4][1].feature_importances_\n",
    "feature_importances = pd.Series(importances, index=[list(X_train)[i] for i in range(len(importances))]).sort_values(ascending=False)\n",
    "\n",
    "# Visualization\n",
    "feature_importances[0:30].plot(kind='bar', title='Feature Importances')\n",
    "\n",
    "# Score 88.25%'''\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ks_2samp <a class=\"anchor\" id=\"ks_2samp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Features\n",
    "# We can use set().difference() but the order matters\n",
    "# Matters only for deterministic results\n",
    "# In case of remove() we will not change order\n",
    "# even if variable will be renamed\n",
    "# please see this link to see how set is ordered\n",
    "# https://stackoverflow.com/questions/12165200/order-of-unordered-python-sets\n",
    "\n",
    "rm_cols = [\n",
    "    'TransactionID', 'TransactionDT',   # These columns are pure noise right now\n",
    "    'isFraud',                          # Not target in features))\n",
    "    'uid', 'uid2', 'uid3',              # Our new client uID -> very noisy data\n",
    "    'bank_type',                        # Victims bank could differ by time\n",
    "    'DT', 'DT_M', 'DT_W', 'DT_D',       # Temporary Variables\n",
    "  #  'DT_hour', 'DT_day_week', 'DT_day',\n",
    "    'DT_D_total', 'DT_W_total', 'DT_M_total',\n",
    "    'id_30', 'id_31', 'id_33',\n",
    "]\n",
    "\n",
    "'''features_check = []\n",
    "columns_to_check = set(list(X_train)).difference(set(list(X_test)) + rm_cols)\n",
    "print('columns_to_check:', columns_to_check)\n",
    "for i in columns_to_check:\n",
    "    features_check.append(ks_2samp(X_test[i], X_train[i])[1])\n",
    "\n",
    "features_check = pd.Series(\n",
    "    features_check, index=columns_to_check).sort_values()\n",
    "features_discard = list(features_check[features_check == 0].index)\n",
    "print('features_discard:',features_discard)'''\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Features\n",
    "# We can use set().difference() but the order matters\n",
    "# Matters only for deterministic results\n",
    "# In case of remove() we will not change order\n",
    "# even if variable will be renamed\n",
    "# please see this link to see how set is ordered\n",
    "# https://stackoverflow.com/questions/12165200/order-of-unordered-python-sets\n",
    "\n",
    "# Final features list\n",
    "features_columns = [col for col in list(X_test) if col not in rm_cols]\n",
    "features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Results <a class=\"anchor\" id=\"Kaggle\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.643Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_regression_clasterization = pd.concat([y_train, X_train], axis=1)\n",
    "test_df_regression_clasterization = X_test\n",
    "\n",
    "print(train_df_regression_clasterization.shape)\n",
    "print(test_df_regression_clasterization.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model params\n",
    "lgb_params = {\n",
    "                    'objective': 'binary',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'metric': 'auc',\n",
    "                    'n_jobs': -1,\n",
    "                    'learning_rate': 0.005,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth': -1,\n",
    "                    'tree_learner': 'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq': 1,\n",
    "                    'subsample': 0.7,\n",
    "                    'n_estimators': 1800,\n",
    "                    'max_bin': 255,\n",
    "                    'verbose': -1,\n",
    "                    'seed': 66,\n",
    "                    'early_stopping_rounds': 100,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.663Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "test_predictions, model = make_predictions(train_df_regression_clasterization, test_df_regression_clasterization, features_columns, 'isFraud', lgb_params, NFOLDS=10)\n",
    "\n",
    "print('Total time: {}'.format(SecondsToStr(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.672Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions['isFraud'] = test_predictions['prediction']\n",
    "test_predictions[['TransactionID','isFraud']].to_csv('test_{}.csv'.format('20.09.2019'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-18T10:22:54.681Z"
    }
   },
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
